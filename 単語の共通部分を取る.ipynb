{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "entitled-steal",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('suihanki_wakati.txt', mode='r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loose-oasis",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "conservative-fundamental",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 名詞が連続してたら名詞句として扱う(わかち書かない)\n",
    "# 名詞・名詞句以外は削除する\n",
    "def preprocess2(text):\n",
    "    from szk_lib import hinshi_check\n",
    "    \n",
    "    # 名詞と名詞句とを格納する集合\n",
    "    nouns = set()\n",
    "    \n",
    "    # i文目のj語目が格納された二次元配列sentence(i, j)\n",
    "    sentence = text.split('\\n')\n",
    "    for i in range(len(sentence)):\n",
    "        sentence[i] = sentence[i].split()\n",
    "\n",
    "    for i in range(len(sentence)):\n",
    "        j = 0\n",
    "        while j < len(sentence[i]):\n",
    "            if hinshi_check(sentence[i][j]) == '名詞':\n",
    "                w = ''\n",
    "                while j<len(sentence[i]) and hinshi_check(sentence[i][j])=='名詞':\n",
    "                    w += sentence[i][j]\n",
    "                    j += 1\n",
    "                nouns.add(w)\n",
    "            j += 1\n",
    "\n",
    "    return list(nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "driven-yugoslavia",
   "metadata": {},
   "outputs": [],
   "source": [
    "# s1, s2 の最長共通部分を返す\n",
    "def find_longest_substr(s1, s2):\n",
    "    for length in range(len(s2), 0, -1):\n",
    "        for p0 in range(len(s2) - length + 1):\n",
    "            substr = s2[p0:(p0 + length)]\n",
    "            if substr in s1:\n",
    "                return(substr)\n",
    "    return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "adequate-allowance",
   "metadata": {},
   "outputs": [],
   "source": [
    "def substr(text):\n",
    "    # textから名詞と名詞句だけ抽出\n",
    "    nouns = preprocess2(text)\n",
    "\n",
    "    # 「s1,s2の共通部分, s1, s2」をtxtに保存\n",
    "    for i in range(len(nouns)):\n",
    "        for j in range(i+1, len(nouns)):\n",
    "            c = find_longest_substr(nouns[i], nouns[j])\n",
    "            if len(c) >= 2:\n",
    "                with open('共通部分.txt', mode='a', encoding='utf-8') as f:\n",
    "                    f.write(', '.join([c, nouns[i], nouns[j]]))\n",
    "                    f.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affiliated-madonna",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "helpful-breathing",
   "metadata": {},
   "outputs": [],
   "source": [
    "substr(text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
