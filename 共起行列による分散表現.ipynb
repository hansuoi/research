{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "assisted-photography",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('suihanki_wakati.txt', mode='r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liquid-arrest",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "presidential-mission",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 共起行列(と単語のリスト)を返す\n",
    "# w_size: ウィンドウ幅\n",
    "def cooccurrence(text, w_size):\n",
    "    import numpy as np\n",
    "\n",
    "    # i文目のj語目が格納された二次元配列sentence(i, j)\n",
    "    sentence = text.split('\\n')\n",
    "    for i in range(len(sentence)):\n",
    "        sentence[i] = sentence[i].split()\n",
    "\n",
    "    # 登場した単語のリスト\n",
    "    words = list(set(text.split()))\n",
    "\n",
    "    # words数サイズのゼロ行列を作る\n",
    "    matrix = np.zeros((len(words), len(words)))\n",
    "\n",
    "    # number番目の単語がw\n",
    "    for number, w in enumerate(words):\n",
    "        for i in range(len(sentence)):\n",
    "            for j in range(len(sentence[i])):\n",
    "                # wとi文目j単語目とが合致したら\n",
    "                if w == sentence[i][j]:\n",
    "                    # 行列の、窓幅分前の単語のところを+1する\n",
    "                    for d in range(1, w_size+1):\n",
    "                        if j-d >= 0:\n",
    "                            matrix[number][words.index(sentence[i][j-d])] += 1\n",
    "                    # 行列の、窓幅分後の単語のところを+1する\n",
    "                    for d in range(1, w_size+1):\n",
    "                        if j+d < len(sentence[i]):\n",
    "                            matrix[number][words.index(sentence[i][j+d])] += 1\n",
    "\n",
    "    return words, matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vital-mistake",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "considered-combat",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ベクトルの0でない成分を返す\n",
    "# wは単語ベクトル\n",
    "def F(w):\n",
    "    a = []\n",
    "    for n, i in enumerate(w):\n",
    "        if i != 0:\n",
    "            a.append(n)\n",
    "    return set(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "sporting-attention",
   "metadata": {},
   "outputs": [],
   "source": [
    "def WeedsP(w1, w2):\n",
    "    m = 0\n",
    "    for i in list(F(w1)):\n",
    "        m += w1[i]\n",
    "    \n",
    "    if m == 0:\n",
    "        return 0\n",
    "\n",
    "    c = 0\n",
    "    for i in list(F(w1)&F(w2)):\n",
    "        c += w1[i]\n",
    "    return c / m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "agreed-compound",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "italian-perception",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weeds_list(text, w_size=1):\n",
    "    # words: textに登場する単語のリスト\n",
    "    # matrix: 共起行列\n",
    "    words, matrix = cooccurrence(text, w_size=1)\n",
    "    \n",
    "    # words[i]が下位・words[j]が上位、と仮定してweedsを計算する\n",
    "    for i in range(len(words)):\n",
    "        for j in range(len(words)):\n",
    "            if i != j:\n",
    "                w = WeedsP(matrix[i], matrix[j])\n",
    "                if w >= 0.5:\n",
    "                    with open('weeds.txt', mode='a', encoding='utf-8') as f:\n",
    "                        f.write(', '.join([words[i], words[j], str(w)]))\n",
    "                        f.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solved-bonus",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unauthorized-opening",
   "metadata": {},
   "source": [
    "共起行列作る関数を直してもう少し早くする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "handmade-possession",
   "metadata": {},
   "outputs": [],
   "source": [
    "def c_matrix(text, w_size, w_list):\n",
    "    import numpy as np\n",
    "    #import pickle\n",
    "    from szk_lib import genkei\n",
    "\n",
    "    # i文目のj語目が格納された二次元配列sentence(i, j)\n",
    "    sentence = text.split('\\n')\n",
    "    for i in range(len(sentence)):\n",
    "        sentence[i] = sentence[i].split()\n",
    "\n",
    "    # 登場した単語(原形)のリスト\n",
    "    #with open(w_list, mode='rb') as f:\n",
    "    #    words = pickle.load(f)\n",
    "    words = w_list\n",
    "\n",
    "    # words数サイズのゼロ行列を作る\n",
    "    matrix = np.zeros((len(words), len(words)))\n",
    "\n",
    "    for i in range(len(sentence)):\n",
    "        for j in range(len(sentence[i])):\n",
    "            number = words.index(genkei(sentence[i][j]))\n",
    "            # 行列の、窓幅分前の単語のところを+1する\n",
    "            for d in range(1, w_size+1):\n",
    "                if j-d >= 0:\n",
    "                    matrix[number][words.index(genkei(sentence[i][j-d]))] += 1\n",
    "                if j+d < len(sentence[i]):\n",
    "                    matrix[number][words.index(genkei(sentence[i][j+d]))] += 1\n",
    "\n",
    "    #with open('c_matrix', mode='wb') as f:\n",
    "    #    pickle.dump(matrix , f)\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "impressive-statement",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "トンネル を 抜ける と そこ は 雪国 だっ た 恥 の 多い 生涯 を 送っ て き まし た \n",
      "\n",
      "['トンネル', 'は', 'そこ', '生涯', 'き', '多い', '恥', '抜ける', '雪国', 'だ', 'ます', '送る', 'て', 'と', 'を', 'の', 'た']\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "text = 'トンネルを抜けるとそこは雪国だった\\n恥の多い生涯を送ってきました'\n",
    "import MeCab\n",
    "import szk_lib\n",
    "tagger = MeCab.Tagger(\"-Owakati\")\n",
    "tagger.parse('')\n",
    "text = tagger.parse(text)\n",
    "words = list(set(text.split()))\n",
    "for i in range(len(words)):\n",
    "    words[i] = szk_lib.genkei(words[i])\n",
    "words = list(set(words))\n",
    "print(text)\n",
    "print(words)\n",
    "print(c_matrix(text, 1, words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "chinese-twenty",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 原形入手・改善版\n",
    "def genkei(word):\n",
    "    import MeCab\n",
    "    tagger = MeCab.Tagger(\"-Ochasen\")\n",
    "    tagger.parse('')\n",
    "    node = tagger.parseToNode(word)\n",
    "\n",
    "    while node:\n",
    "        word = node.surface\n",
    "        wclass = node.feature.split(',')\n",
    "        # wclass:品詞,品詞細分類1,品詞細分類2,品詞細分類3,活用形,活用型,原形,読み,発音\n",
    "\n",
    "        if wclass[0] != u'BOS/EOS':\n",
    "            if wclass[6] == None:\n",
    "                return word\n",
    "            else:\n",
    "                return wclass[6]\n",
    "        node = node.next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "numerical-acceptance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 品詞入手・改善版\n",
    "def hinshi(word):\n",
    "    import MeCab\n",
    "    tagger = MeCab.Tagger(\"-Ochasen\")\n",
    "    tagger.parse('')\n",
    "    node = tagger.parseToNode(genkei(word))\n",
    "\n",
    "    while node:\n",
    "        word = node.surface\n",
    "        wclass = node.feature.split(',')\n",
    "        # wclass:品詞,品詞細分類1,品詞細分類2,品詞細分類3,活用形,活用型,原形,読み,発音\n",
    "\n",
    "        if wclass[0] != u'BOS/EOS':\n",
    "            return wclass[0]\n",
    "        node = node.next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "convertible-entertainment",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "帰る\n",
      "動詞\n"
     ]
    }
   ],
   "source": [
    "print(genkei('帰り'))\n",
    "print(hinshi('帰り'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
